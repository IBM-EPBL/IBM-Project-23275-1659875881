# -*- coding: utf-8 -*-
"""Sprint 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d8-1TElknLc5ypqzsAnhbUwu4k9PvEMY

Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import sklearn
import scipy
import seaborn as sb

"""Import Dataset"""

data=pd.read_csv("weather.csv")

"""Analyze the data

Handling Missing Data
"""

with pd.option_context('mode.use_inf_as_null', True):
   data = data.dropna()

data.isnull()

data.isnull().sum()

data['MinTemp'].fillna(data['MinTemp'].mean(),inplace=True)
data['MaxTemp'].fillna(data['MaxTemp'].mean(),inplace=True)
data['Rainfall'].fillna(data['Rainfall'].mean(),inplace=True)
data['WindGustSpeed'].fillna(data['WindGustSpeed'].mean(),inplace=True)
data['WindSpeed9am'].fillna(data['WindSpeed9am'].mean(),inplace=True)
data['WindSpeed3pm'].fillna(data['WindSpeed3pm'].mean(),inplace=True)
data['Humidity3pm'].fillna(data['Humidity3pm'].mean(),inplace=True)
data['Humidity9am'].fillna(data['Humidity9am'].mean(),inplace=True)
data['Temp9am'].fillna(data['Temp9am'].mean(),inplace=True)
data['Temp3pm'].fillna(data['Temp3pm'].mean(),inplace=True)
data['Evaporation'].fillna(data['Evaporation'].mean(),inplace=True)
data['Sunshine'].fillna(data['Sunshine'].mean(),inplace=True)
data['Cloud9am'].fillna(data['Cloud9am'].mean(),inplace=True)
data['Cloud3pm'].fillna(data['Cloud3pm'].mean(),inplace=True)

data.head()

"""Feature Scaling"""

data_c=data[["RainToday","WindGustDir","WindDir9am","WindDir3pm","RainTomorrow"]]

data.drop(columns=["Evaporation","Sunshine","Cloud9am","Cloud3pm"],axis=1,inplace=True)
data.drop(columns=["RainToday","WindGustDir","WindDir9am","WindDir3pm","RainTomorrow"],axis=1,inplace=True)

data.drop(columns=["Date"],axis=1,inplace=True)

data.drop(columns=["Location"],axis=1,inplace=True)

c_names=data_c.columns

from sklearn.impute import SimpleImputer

imp_mode=SimpleImputer(missing_values=np.nan,strategy="most_frequent")

data_c=imp_mode.fit_transform(data_c)

data_c=pd.DataFrame(data_c,columns=c_names)

data_c["RainToday"]=data_c["RainToday"].astype('category')
data_c["WindGustDir"]=data_c["WindGustDir"].astype('category')
data_c["WindDir9am"]=data_c["WindDir9am"].astype('category')
data_c["WindDir3pm"]=data_c["WindDir3pm"].astype('category')
data_c["RainTomorrow"]=data_c["RainTomorrow"].astype('category')

data_c["RainToday"]=data_c["RainToday"].cat.codes
data_c["WindGustDir"]=data_c["WindGustDir"].cat.codes
data_c["WindDir9am"]=data_c["WindDir9am"].cat.codes
data_c["WindDir3pm"]=data_c["WindDir3pm"].cat.codes
data_c["RainTomorrow"]=data_c["RainTomorrow"].cat.codes

data_c.tail()

data=pd.concat([data,data_c],axis=1)

data.head()

"""Independant and Dependant Variables"""

from sklearn.preprocessing import LabelEncoder, MinMaxScaler

y=data['RainTomorrow']
x=data.drop('RainTomorrow',axis=1)

names=x.columns
names

LE = LabelEncoder()
x['RainToday'] = LE.fit_transform(x['RainToday'])

LE = LabelEncoder()
x['WindGustDir'] = LE.fit_transform(x['WindGustDir'])

LE = LabelEncoder()
x['WindDir3pm'] = LE.fit_transform(x['WindDir3pm'])

LE = LabelEncoder()
y = LE.fit_transform(y)

from sklearn.preprocessing import StandardScaler

sc=StandardScaler()

x = sc.fit_transform(x)

x = pd.DataFrame(x,columns=names)
y  =pd.DataFrame(y)

from sklearn import model_selection

x_train,x_test,y_train,y_test = model_selection.train_test_split(x,y,test_size = 0.2,random_state = 0)

x_train.fillna(x_train.mean(), inplace=True)
x_test.fillna(x_test.mean(), inplace=True)
y_train.fillna(y_train.mean(), inplace=True)
y_test.fillna(x_train.mean(), inplace=True)

np.any(np.isnan(x_train))
np.all(np.isfinite(x_train))

"""SPRINT 2

Model Comparsion
"""

import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier

XGBoost = xgb.XGBRFClassifier()

Rand_forest = sklearn.ensemble.RandomForestClassifier()

svm = sklearn.svm.SVC()

Dtree=sklearn.tree. DecisionTreeClassifier()

GBM = sklearn.ensemble.GradientBoostingClassifier()

log=sklearn.linear_model.LogisticRegression()

np.all(np.isfinite(x_train))

XGBoost.fit(x_train,y_train) 
Rand_forest.fit(x_train,y_train.values.ravel()) 
svm.fit(x_train,y_train.values.ravel())
Dtree.fit(x_train,y_train.values.ravel()) 
GBM.fit(x_train,y_train.values.ravel()) 
log.fit(x_train,y_train.values.ravel())

p1 = XGBoost.predict(x_train) 
p2 = Rand_forest.predict(x_train)
p3 = svm.predict(x_train)
p4 = Dtree.predict(x_train)
p5 = GBM.predict(x_train)
p6 = log.predict(x_train)

from sklearn import metrics
print("xgboost:",metrics.accuracy_score (y_train,p1)) 
print("Rand_forest: ", metrics.accuracy_score (y_train,p2)) 
print("svm:",metrics.accuracy_score (y_train, p3)) 
print("Dtree:", metrics.accuracy_score (y_train,p4)) 
print("GBM:",metrics.accuracy_score (y_train,p5)) 
print("log: ",metrics.accuracy_score (y_train, p6))

"""Model Evaluation"""

y_pred = Rand_forest.predict(x_test)
conf_matrix= metrics.confusion_matrix(y_test,y_pred)
conf_matrix

fig, ax = plt.subplots(figsize=(7.5,7.5))
ax.matshow(conf_matrix,alpha=0.3)
for i in range(conf_matrix.shape[0]):
  for j in range(conf_matrix.shape[1]):
    ax.text(x=j,y=i,s=conf_matrix[i,j],va='center',ha='center',size='xx-large')
plt.xlabel('Predictions', fontsize=18) 
plt.ylabel('Actuals', fontsize=18) 
plt.title('Confusion Matrix', fontsize=18)
plt.show()

print(conf_matrix)
Accuracy=metrics.accuracy_score (y_train,p2)
print("Accuracy:",Accuracy)
Precision=metrics.precision_score(y_test, y_pred,average='macro')
print("Precession:",Precision)
Recall=metrics.recall_score(y_test, y_pred,average='macro')
print("Recall:",Recall)
F1_score=metrics.f1_score(y_test, y_pred,average='macro')
print("F1-score:",F1_score)

import pickle
model = Rand_forest
pickle.dump(model,open('rainfall.pkl','wb')) 
le = LE
pickle.dump(le,open('encoder.pkl','wb')) 
pickle.dump(imp_mode,open('impter.pkl','wb'))
pickle.dump(sc,open('scale.pkl','wb'))